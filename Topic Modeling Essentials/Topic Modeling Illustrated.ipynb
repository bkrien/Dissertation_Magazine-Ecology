{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Modeling Illustrated\n",
    "\n",
    "This Jupyter notebooks provides an accessible explanation and overview of some of the topic modeling methods that are used in my dissertation project. This notebook can be downloaded and manipulated to adapt or recreate any of my analyses. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "Topic modeling is a method of extrapolationg a set of semantic patterns or topics from a text or set of texts (a *corpus*) that can be used to create models of texts and what they are about. These topics take the form of sets of words that are mathematically related to one another based on their appearance within the text (One of the key assumptions of topic modeling is that there is a dgree of congruency between the mathematical relationships within the text and the semantic ones). As Ted Underwood has pointed out, these topics are not really topics in the conventional sense, but rather *discourses* that are defined by shared language and can be understood and inferred using probability. \n",
    "\n",
    "It's important to note that while topic modeling involves \"exact science\" in its mathematical analysis, it also involves a great deal of judgment, input, and interpretation on the part of the user to extract meaningful results. Thus, it might be more accurately referred to as an exacting art rather than an exact science. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>How Does It Work?</h2>\n",
    "\n",
    "The best analogy that I have for topic modeling is the <b>creating the seating chart at a wedding</b>. \n",
    "\n",
    "Imagine a wedding as is a concatenation of the various branches of the families and friend groups involved (the texts), each of which consist of individual people (words). You and your fiance have a very accomodating venue that will allow you as many or as few tables as you like and have to try and arrange the tables (topics) so that you have the greatest possible compatibility at each table. Looking at your large and very heterogeneous guest list, you're not sure where to start, so you just begin randomly assigning people (words) to a number of tables (topics) that seem to make sense. You then go through and start rearranging each person based on commonalities (frequency of appearing together) which may not have been obvious at first glance, such as your third cousin and your fiance's shared passion for Civil War reenacting. \n",
    "\n",
    "You repeat this process again and again and again, going over each guest's placement to maximize the compatibility at each table, not necessarily concerned with whose family they hail from. If the tables just don't seem to work, you can also go back to the beginning and change the number of tables to one that seems like it will work better. Eventually you will find a combination that seems to work well for nearly everyone. There will likely be logical and obvious pairings as the various members of each familial or friend group will tend to have more of a shared history, but there will be many overlaps like your brother and his spouse, who met at the University of Wisconsin, ending up at a table with other Wisconsin alumni, most of whom are friends of your fiance. Other tables, however, might involve combinations that you would not have at first imagined; the Civil War reenacters, for example, end up spending the evening at a table with an aunt who teaches AP U.S. History and several of your fiance's colleagues from the history department in deep conversation about Grant's strategy during the Overland Campaign. \n",
    "\n",
    "The great benefit of topic modeling over wedding planning, aside from the cost, is that words tend to be simpler than people and that you can <i>measure</i> the compatibility of the topics. \n",
    "\n",
    "Something about digital sausage making perhaps\n",
    "Unpacking the black box of a digital dissertation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Started\n",
    "\n",
    "I'll walk through the steps involved in topic modeling to help illustrate the process and how the code works. Note that this code can be downloaded and will facilitate interaction. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/bradykrien/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# It's necessary to import a variety of code packages or libraries to \n",
    "#manipulate the text in various ways and run the analyses\n",
    "\n",
    "import os\n",
    "import re\n",
    "from htrc_features import feature_reader #make sure that you have the latest version of the htrc-feature-reader\n",
    "from htrc_features import Volume\n",
    "from htrc_features import utils\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "wnl = WordNetLemmatizer()\n",
    "import gensim\n",
    "from gensim import models, corpora, utils, parsing, similarities\n",
    "from gensim.models.ldamulticore import LdaMulticore\n",
    "import pyLDAvis.gensim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Volume: Harper's new monthly magazine. (1900) by Alden, Henry Mills 1836-1919 ed.>\n",
      "<Volume: Harper's new monthly magazine. (1900) by Alden, Henry Mills 1836-1919 ed.>\n",
      "<Volume: Harper's new monthly magazine. (1900) by Alden, Henry Mills 1836-1919 ed.>\n",
      "<Volume: Harper's new monthly magazine. (1900) by Alden, Henry Mills 1836-1919 ed.>\n"
     ]
    }
   ],
   "source": [
    "# Access files\n",
    "# I rely on the Extracted Features dataset from the HathiTrust Research Library. \n",
    "# This open data repository keeps page-level data for the magazines that I work with. \n",
    "# I've stored these files locally in this directory, but they (and the rest of the texts that I work with) can be \n",
    "# acccessed through the HathiTrust\n",
    "\n",
    "#Identify files using their HathiTrust identification number and zipped fill structure\n",
    "htids = [\n",
    "    'uc1.31175023709325',\n",
    "             'uc1.31175023709333',\n",
    "             'uc1.31175023709341',\n",
    "             'uc1.31175023709358'\n",
    "]\n",
    "\n",
    "for htid in htids:\n",
    "    vol = Volume(htid)\n",
    "    print(vol)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
