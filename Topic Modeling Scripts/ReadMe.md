## Topic Modeling Script ReadMe

These scripts were developed to utilize the Gensim ldaMulticore library, a parallelized library for completing topic modeling using parallel computing. In this case, they were written to run on the University of Iowa's Argon High Performance Computer (HPC) Cluter. 

The topic modeling scripts are designed to function in tandem with the MulticoreImplementation.py file pulling the parameters for each topic modeling run from the MulticoreInput.py file. These inputs include the name and description of the particular run (to be included in the documentation that is generated each time the MulticoreImplementation Script is run) along with the HathiTrust IDs of the files that the topic model is to by trained on, and the hyperparameters for the topic model (for more information on the specific hyperparameters, see the Gensim ldaMulticore documentation linked below).

The MulticoreImplmentation.py script generates a number of files (and organizes them within my specific Argon directory based on the date that the run was undertaken. This documentation includes: 
- A run report that details about the topic modeling run that was completing including: 
  - A name and description for the run
  - The date, time, and duration of the run
  - The HathiTrust IDs of the volumes included in the run
  - The titles of the volumes included in the training run 
  - The hyperparameters for the training included in the run, including the number of "workers"
- A text file with a list of the top 20 words associated with each of the topics generated by the training run
- The dictionary and other gensim files that are produced and utilized during the training process
- An interactive HTML visualization of the topics developed through the pyLDAvis library

The MulticoreImplementation utilizes the Extracted Features library to access the non-consumptive, page-level word lists for each of the volumes being used. These are imported as pandas dataframes that are then processed through a variety of natural language processing techniques (including lemmatization, the removal of stopwords, and the removal or non-alphanumeric characters) to produce the final list of tokens that the topic model is trained on. This processing utilizes several libraries, including the Natural Language Toolkit (NLTK). 

Additional Information and Documentation
- Gensim ldaMulticore library can be found here: https://radimrehurek.com/gensim/models/ldamulticore.html
- Argon High Performance Computing Cluster can be found here: https://wiki.uiowa.edu/display/hpcdocs/Argon+Cluster
- HathiTrust Research Center Extracted Features Dataset: https://www.hathitrust.org/feature_extraction_alpha_release
- The Natural Langauge Toolkit: https://www.nltk.org/
- The pyLDAvis library used to create the interactive visualization: https://pypi.org/project/pyLDAvis/
